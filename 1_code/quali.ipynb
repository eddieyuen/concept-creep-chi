{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List the 20 most similar word neighbors in each year for each concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1979 done\n",
      "1980 done\n",
      "1981 done\n",
      "1982 done\n",
      "1983 done\n",
      "1984 done\n",
      "1985 done\n",
      "1986 done\n",
      "1987 done\n",
      "1988 done\n",
      "1989 done\n",
      "1990 done\n",
      "1991 done\n",
      "1992 done\n",
      "1993 done\n",
      "1994 done\n",
      "1995 done\n",
      "1996 done\n",
      "1997 done\n",
      "1998 done\n",
      "1999 done\n",
      "2000 done\n",
      "2001 done\n",
      "2002 done\n",
      "2003 done\n",
      "2004 done\n",
      "2005 done\n",
      "2006 done\n",
      "2007 done\n",
      "2008 done\n",
      "2009 done\n",
      "2010 done\n",
      "2011 done\n",
      "2012 done\n",
      "2013 done\n",
      "2014 done\n",
      "2015 done\n",
      "2016 done\n",
      "2017 done\n",
      "2018 done\n",
      "2019 done\n",
      "2020 done\n",
      "2021 done\n",
      "2022 done\n",
      "2023 done\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "model_folder = '/Users/kawaiyuen/nlpworkshop/concept-creep-chi_raw/models/'\n",
    "concepts_file = '/Users/kawaiyuen/nlpworkshop/concept-creep-chi/0_data/wordlist/concepts.json'\n",
    "output_csv = '/Users/kawaiyuen/nlpworkshop/concept-creep-chi/2_pipeline/preprocessed/quali/by-year.csv'\n",
    "\n",
    "with open(concepts_file, 'r') as file:\n",
    "    concepts = json.load(file)\n",
    "\n",
    "# Create an empty dictionary to store the results\n",
    "results = {}\n",
    "\n",
    "# Iterate over each year\n",
    "for year in range(1979, 2024):\n",
    "\n",
    "    # Create an empty list to store the word neighbors for each concept\n",
    "    year_results = []\n",
    "\n",
    "    # Load the word2vec model\n",
    "    model = Word2Vec.load(model_folder + f'pd_{year}.model')\n",
    "    \n",
    "    # Iterate over each target concept\n",
    "    for key, value in concepts.items():\n",
    "        concept = value[0]\n",
    "\n",
    "        # Check if the concept is present in the word2vec model's vocabulary\n",
    "        if concept in model.wv.key_to_index:\n",
    "            # Find the 10 most similar word neighbors for the concept\n",
    "            neighbors = model.wv.most_similar(concept, topn=50)\n",
    "            \n",
    "            # Extract just the words from the word neighbors\n",
    "            words = [word for word, _ in neighbors]\n",
    "            \n",
    "            # Append the list of words to the year_results list\n",
    "            year_results.append(words)\n",
    "        else:\n",
    "            # If the concept is not present, append an empty list to year_results\n",
    "            year_results.append([])\n",
    "    \n",
    "    # Add the year_results list to the results dictionary with the year as the key\n",
    "    results[str(year)] = year_results\n",
    "    print(f'{year} done')\n",
    "\n",
    "with open(output_csv, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    \n",
    "    # Write the column headers\n",
    "    writer.writerow(['Year'] + list(concepts.keys()))\n",
    "    \n",
    "    # Write the data rows\n",
    "    for year, year_results in results.items():\n",
    "        writer.writerow([year] + year_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List the most similar word neighbors in each five-year timespan for each concept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate lists in each timespan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Define the time ranges\n",
    "time_ranges = ['1979-1983', '1984-1988', '1989-1993', '1994-1998', '1999-2003', '2004-2008', '2009-2013', '2014-2018', '2019-2023']\n",
    "\n",
    "# Create a dictionary to store the concatenated word neighbors for each target concept\n",
    "concatenated_neighbors = {}\n",
    "\n",
    "# Read the CSV file\n",
    "with open('/Users/kawaiyuen/nlpworkshop/concept-creep-chi/2_pipeline/preprocessed/quali/by-year.csv', 'r') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    \n",
    "    # Iterate over each row in the CSV file\n",
    "    for row in reader:\n",
    "        year = int(row['Year'])\n",
    "        \n",
    "        # Check which time range the year falls into\n",
    "        for i, time_range in enumerate(time_ranges):\n",
    "            start_year = int(time_range.split('-')[0])\n",
    "            end_year = int(time_range.split('-')[1])\n",
    "            \n",
    "            # If the year falls within the current time range\n",
    "            if start_year <= year <= end_year:\n",
    "                \n",
    "                # Iterate over each target concept\n",
    "                for concept in row.keys():\n",
    "                    if concept != 'Year':\n",
    "                        neighbors = eval(row[concept])  # Convert the string representation of list to a list\n",
    "                        if concept not in concatenated_neighbors:\n",
    "                            concatenated_neighbors[concept] = [''] * len(time_ranges)\n",
    "                        concatenated_neighbors[concept][i] += ' '.join(neighbors) + ' '\n",
    "\n",
    "# Write the results to a new CSV file\n",
    "with open('/Users/kawaiyuen/nlpworkshop/concept-creep-chi/2_pipeline/preprocessed/quali/by-five-year.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    \n",
    "    # Write the header row\n",
    "    header_row = ['timespan'] + list(concatenated_neighbors.keys())\n",
    "    writer.writerow(header_row)\n",
    "    \n",
    "    # Write the data rows\n",
    "    for i, time_range in enumerate(time_ranges):\n",
    "        data_row = [time_range] + [concatenated_neighbors[concept][i] for concept in concatenated_neighbors.keys()]\n",
    "        writer.writerow(data_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter by at least 3 occurrence in each timespan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv('/Users/kawaiyuen/nlpworkshop/concept-creep-chi/2_pipeline/preprocessed/quali/by-five-year.csv')\n",
    "\n",
    "# Iterate over each cell in the dataframe\n",
    "for index, row in data.iterrows():\n",
    "    for column in data.columns[1:]:\n",
    "        # Split the cell into individual words\n",
    "        words = row[column].split()\n",
    "        \n",
    "        # Count the frequency of each word\n",
    "        word_counts = {}\n",
    "        for word in words:\n",
    "            word_counts[word] = word_counts.get(word, 0) + 1\n",
    "        \n",
    "        # Filter and retain words that appear at least three times\n",
    "        filtered_words = [word for word, count in word_counts.items() if count >= 3]\n",
    "        \n",
    "        # Join the filtered words back into a single string\n",
    "        # filtered_cell = ' '.join(filtered_words)\n",
    "        \n",
    "        # Update the cell in the dataframe\n",
    "        data.at[index, column] = filtered_words\n",
    "\n",
    "# Save the updated dataframe to a new CSV file\n",
    "data.to_csv('/Users/kawaiyuen/nlpworkshop/concept-creep-chi/2_pipeline/preprocessed/quali/by-five-year.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate lists in timespans to one giant list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Create a dictionary to store the concatenated word neighbors for each target concept\n",
    "concatenated_neighbors = {}\n",
    "\n",
    "# Read the CSV file\n",
    "with open('/Users/kawaiyuen/nlpworkshop/concept-creep-chi/2_pipeline/preprocessed/quali/by-five-year.csv', 'r') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    \n",
    "    # Iterate over each row in the CSV file\n",
    "    for row in reader:\n",
    "        \n",
    "        # Iterate over each target concept (excluding the first column)\n",
    "        for concept, neighbors in list(row.items())[1:]:\n",
    "            neighbors = eval(neighbors)  # Convert the string representation of list to a list\n",
    "            \n",
    "            if concept not in concatenated_neighbors:\n",
    "                concatenated_neighbors[concept] = []\n",
    "            \n",
    "            if isinstance(neighbors, list):\n",
    "                concatenated_neighbors[concept].extend(neighbors)\n",
    "            else:\n",
    "                concatenated_neighbors[concept].append(neighbors)\n",
    "\n",
    "# Write the results to a new CSV file\n",
    "with open('/Users/kawaiyuen/nlpworkshop/concept-creep-chi/2_pipeline/preprocessed/quali/one-list.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    \n",
    "    # Write the header row\n",
    "    writer.writerow(concatenated_neighbors.keys())\n",
    "    \n",
    "    # Write the data rows\n",
    "    writer.writerow(concatenated_neighbors.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct final word neighbors lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv('/Users/kawaiyuen/nlpworkshop/concept-creep-chi/2_pipeline/preprocessed/quali/one-list.csv')\n",
    "\n",
    "# Iterate over each cell in the dataframe\n",
    "for index, row in data.iterrows():\n",
    "    for column in data.columns[0:]:\n",
    "        # Split the cell into individual words\n",
    "        words = ast.literal_eval(row[column])\n",
    "\n",
    "        # Count the frequency of each word\n",
    "        word_counts = {}\n",
    "        for word in words:\n",
    "            word_counts[word] = word_counts.get(word, 0) + 1\n",
    "\n",
    "        # Filter and retain words that appear at least three times\n",
    "        filtered_words = [word for word, count in word_counts.items() if count >= 2]\n",
    "        \n",
    "        # Remove unnecessary characters and clean the words\n",
    "        cleaned_words = [filtered_word.strip().strip('\\'\"') for filtered_word in filtered_words]\n",
    "        \n",
    "        # Update the cell in the dataframe\n",
    "        data.at[index, column] = cleaned_words\n",
    "\n",
    "# Save the updated dataframe to a new CSV file\n",
    "data.to_csv('/Users/kawaiyuen/nlpworkshop/concept-creep-chi/2_pipeline/preprocessed/quali/final-list.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import ast\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv('/Users/kawaiyuen/nlpworkshop/concept-creep-chi/2_pipeline/preprocessed/quali/final-list.csv')\n",
    "\n",
    "# Get the headers (target concepts)\n",
    "headers = list(data.columns)\n",
    "\n",
    "# Create an empty dictionary to store the JSON data\n",
    "json_data = {}\n",
    "\n",
    "# Iterate over each header\n",
    "for header in headers:\n",
    "    # Get the word neighbors for the current header\n",
    "    neighbors = data[header].tolist()\n",
    "    \n",
    "    # Remove None values from the list of neighbors\n",
    "    neighbors = [neighbor for neighbor in neighbors if pd.notnull(neighbor)]\n",
    "    \n",
    "    # Clean the structure of the value for the current key\n",
    "    cleaned_neighbors = []\n",
    "    for neighbor in neighbors:\n",
    "        cleaned_neighbor = ast.literal_eval(neighbor)\n",
    "        cleaned_neighbors.extend(cleaned_neighbor)\n",
    "    \n",
    "    # Update the dictionary with the target concept and its cleaned neighbors\n",
    "    json_data[header] = cleaned_neighbors\n",
    "\n",
    "# Convert the dictionary to JSON with specified encoding\n",
    "json_output = json.dumps(json_data, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open('/Users/kawaiyuen/nlpworkshop/concept-creep-chi/0_data/wordlist/concepts_neighbors.json', 'w', encoding='utf-8') as file:\n",
    "    file.write(json_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
